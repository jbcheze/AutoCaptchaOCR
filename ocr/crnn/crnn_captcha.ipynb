{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:03.773913Z",
     "start_time": "2026-01-26T21:17:01.692262Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f341984f-d5ab-427c-8c1f-ff0dd1265745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddfaa2ad080ff3f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:03.784330Z",
     "start_time": "2026-01-26T21:17:03.779070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 527\n",
      "Valid: 132\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\jbche\\OneDrive - Université Paris 1 Panthéon-Sorbonne\\MOSEF\\projets\\webscrapping\\projet\\data\\data_OCR_Captcha-20260117T105614Z-1-001\\finetune_russie\\data_russie\"\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VALID_DIR = os.path.join(DATA_DIR, \"valid\")\n",
    "\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VALID_DIR, exist_ok=True)\n",
    "\n",
    "# liste des fichiers images (ajuste extensions si besoin)\n",
    "files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "\n",
    "random.shuffle(files)\n",
    "\n",
    "split = int(0.8 * len(files))\n",
    "train_files = files[:split]\n",
    "valid_files = files[split:]\n",
    "\n",
    "for f in train_files:\n",
    "    shutil.move(os.path.join(DATA_DIR, f), os.path.join(TRAIN_DIR, f))\n",
    "\n",
    "for f in valid_files:\n",
    "    shutil.move(os.path.join(DATA_DIR, f), os.path.join(VALID_DIR, f))\n",
    "\n",
    "print(\"Train:\", len(os.listdir(TRAIN_DIR)))\n",
    "print(\"Valid:\", len(os.listdir(VALID_DIR)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58426ee2cc725b52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:03.798641Z",
     "start_time": "2026-01-26T21:17:03.789310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228.png</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23eu.png</td>\n",
       "      <td>23eu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23hq.png</td>\n",
       "      <td>23hq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24u.png</td>\n",
       "      <td>24u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24y6v.png</td>\n",
       "      <td>24y6v</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name   text\n",
       "0    228.png    228\n",
       "1   23eu.png   23eu\n",
       "2   23hq.png   23hq\n",
       "3    24u.png    24u\n",
       "4  24y6v.png  24y6v"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "train_path = r\"C:\\Users\\jbche\\OneDrive - Université Paris 1 Panthéon-Sorbonne\\MOSEF\\projets\\webscrapping\\projet\\data\\data_OCR_Captcha-20260117T105614Z-1-001\\finetune_russie\\data_russie\\train\"\n",
    "\n",
    "train_dir_list = os.listdir(train_path)\n",
    "\n",
    "train_df = pd.DataFrame(train_dir_list, columns=[\"file_name\"])\n",
    "\n",
    "train_df[\"text\"] = train_df[\"file_name\"].map(\n",
    "    lambda x: x.split(\".\")[0]\n",
    ")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ab826f83f46a71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:03.830216Z",
     "start_time": "2026-01-26T21:17:03.826321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28p.png</td>\n",
       "      <td>28p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2du.png</td>\n",
       "      <td>2du</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2gmsg.png</td>\n",
       "      <td>2gmsg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2s6x3.png</td>\n",
       "      <td>2s6x3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34bm.png</td>\n",
       "      <td>34bm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name   text\n",
       "0    28p.png    28p\n",
       "1    2du.png    2du\n",
       "2  2gmsg.png  2gmsg\n",
       "3  2s6x3.png  2s6x3\n",
       "4   34bm.png   34bm"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_path = r\"C:\\Users\\jbche\\OneDrive - Université Paris 1 Panthéon-Sorbonne\\MOSEF\\projets\\webscrapping\\projet\\data\\data_OCR_Captcha-20260117T105614Z-1-001\\finetune_russie\\data_russie\\valid\"\n",
    "\n",
    "valid_dir_list = os.listdir(valid_path)\n",
    "\n",
    "valid_df = pd.DataFrame(valid_dir_list, columns=[\"file_name\"])\n",
    "\n",
    "valid_df[\"text\"] = valid_df[\"file_name\"].map(\n",
    "    lambda x: x.split(\".\")[0]\n",
    ")\n",
    "\n",
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a49ab19e39ef7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:03.924294Z",
     "start_time": "2026-01-26T21:17:03.919803Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_path = \"/Users/theo/Desktop/data_russie/test\"\n",
    "\n",
    "# test_dir_list = os.listdir(test_path)\n",
    "\n",
    "# test_df = pd.DataFrame(test_dir_list, columns=[\"file_name\"])\n",
    "\n",
    "# test_df[\"text\"] = test_df[\"file_name\"].map(\n",
    "#     lambda x: x.split(\".\")[0]\n",
    "# )\n",
    "\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "717d5dfae8e3baf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:03.959234Z",
     "start_time": "2026-01-26T21:17:03.956312Z"
    }
   },
   "outputs": [],
   "source": [
    "class Captcha_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=10):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        file_name = self.df.iloc[idx][\"file_name\"]\n",
    "        text = self.df.iloc[idx][\"text\"]\n",
    "\n",
    "        image = Image.open(\n",
    "            os.path.join(self.root_dir, file_name)\n",
    "        ).convert(\"RGB\")\n",
    "\n",
    "        pixel_values = self.processor(\n",
    "            image,\n",
    "            return_tensors=\"pt\"\n",
    "        ).pixel_values.squeeze()\n",
    "\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "\n",
    "        labels = [\n",
    "            l if l != self.processor.tokenizer.pad_token_id else -100\n",
    "            for l in labels\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(labels)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12a26b1fd92a275",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:03.994865Z",
     "start_time": "2026-01-26T21:17:03.993136Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"microsoft/trocr-base-printed\"\n",
    "MODEL_NAME = MODEL_CKPT.split(\"/\")[-1] + \"_captcha_ocr\"\n",
    "\n",
    "NUM_OF_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d02c431520e46e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:06.103578Z",
     "start_time": "2026-01-26T21:17:04.006802Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "MODEL_CKPT = \"microsoft/trocr-base-printed\"\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3df251d2728cb54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:06.164096Z",
     "start_time": "2026-01-26T21:17:06.162348Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = Captcha_Dataset(\n",
    "    root_dir=train_path,\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "valid_ds = Captcha_Dataset(\n",
    "    root_dir=valid_path,\n",
    "    df=valid_df,\n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "# test_ds = Captcha_Dataset(\n",
    "#     root_dir=test_path,\n",
    "#     df=test_df,\n",
    "#     processor=processor\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad82effc376d6bd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:06.181964Z",
     "start_time": "2026-01-26T21:17:06.180188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 527 samples in it.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The training dataset has {len(train_ds)} samples in it.\")\n",
    "# print(f\"The testing dataset has {len(test_ds)} samples in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbb8b62ee108cf67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:06.234659Z",
     "start_time": "2026-01-26T21:17:06.191879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values  :  torch.Size([3, 384, 384])\n",
      "labels  :  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoding = train_ds[10]\n",
    "\n",
    "for k,v in encoding.items():\n",
    "    print(k, \" : \", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd8e1be5acc8a877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:06.265930Z",
     "start_time": "2026-01-26T21:17:06.238882Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(-0.5), np.float64(119.5), np.float64(71.5), np.float64(-0.5))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx = 0\n",
    "\n",
    "img_path = os.path.join(train_path, train_df.iloc[idx][\"file_name\"])\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a50c567705877453",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:06.279287Z",
     "start_time": "2026-01-26T21:17:06.269652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 228\n"
     ]
    }
   ],
   "source": [
    "encoding = train_ds[idx]\n",
    "\n",
    "labels = encoding[\"labels\"].clone()\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(\"Label:\", label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f8ce7efeb9b283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:06.959254Z",
     "start_time": "2026-01-26T21:17:06.282862Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 478/478 [00:01<00:00, 311.45it/s, Materializing param=encoder\n",
      "VisionEncoderDecoderModel LOAD REPORT from: microsoft/trocr-base-printed\n",
      "Key                         | Status  | \n",
      "----------------------------+---------+-\n",
      "encoder.pooler.dense.weight | MISSING | \n",
      "encoder.pooler.dense.bias   | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c92d8857ef9b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 10\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 2\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a903a6196d7e6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{MODEL_NAME}\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    logging_first_step=True,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "\n",
    "    warmup_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=NUM_OF_EPOCHS,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    logging_strategy=\"steps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93908a8-a301-46cf-a801-900c12d19d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77015fc63895d867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T21:17:10.119863Z",
     "start_time": "2026-01-26T21:17:07.005609Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=valid_ds,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33e684b3a29d69e6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2026-01-26T21:17:10.339253Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 20/330 11:57 < 3:26:04, 0.03 it/s, Epoch 0.29/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2174\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2172\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2529\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2530\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2532\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2533\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2534\u001b[0m )\n\u001b[0;32m   2535\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2536\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2539\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2540\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2541\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2542\u001b[0m ):\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2544\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:3837\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3835\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3837\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3839\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\accelerate\\accelerator.py:2852\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2852\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\_tensor.py:630\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    622\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    623\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[1;32m--> 630\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    863\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    866\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    867\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad146093c20713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du modèle et du processor\n",
    "trainer.save_model(f\"./{MODEL_NAME}\")\n",
    "processor.save_pretrained(f\"./{MODEL_NAME}\")\n",
    "\n",
    "print(\"Modèle sauvegardé dans :\", f\"./{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229db7530ba2a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation sur test\n",
    "metrics = trainer.evaluate(test_ds)\n",
    "\n",
    "print(\"Résultats test:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296adc67866512a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_captcha(image_path, model, processor, device=\"cpu\"):\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    pixel_values = processor(\n",
    "        image,\n",
    "        return_tensors=\"pt\"\n",
    "    ).pixel_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "\n",
    "    pred = processor.batch_decode(\n",
    "        generated_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6eef5ce944f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(f\"./{MODEL_NAME}\")\n",
    "processor = TrOCRProcessor.from_pretrained(f\"./{MODEL_NAME}\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"Modèle chargé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16882ecce5ae04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sur 10 images\n",
    "for img in os.listdir(TEST_DIR)[:10]:\n",
    "\n",
    "    path = os.path.join(TEST_DIR, img)\n",
    "\n",
    "    pred = predict_captcha(path, model, processor)\n",
    "\n",
    "    true = img.split(\".\")[0]\n",
    "\n",
    "    print(f\"GT: {true} | Pred: {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5423a2d4-2b6c-422d-b2ab-594fb3da5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (6.33.4)\n",
      "Requirement already satisfied: sentencepiece in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: tiktoken in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (0.12.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (from tiktoken) (2026.1.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\mosef\\webscrapping\\projet\\captchas-automatic-resolution\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U protobuf sentencepiece tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d115e964-c053-43cc-99b5-a3ecac372ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 527\n",
      "Val  : 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 360/360 [00:01<00:00, 188.99it/s, Materializing param=encoder\n",
      "VisionEncoderDecoderModel LOAD REPORT from: microsoft/trocr-small-printed\n",
      "Key                         | Status  | \n",
      "----------------------------+---------+-\n",
      "encoder.pooler.dense.bias   | MISSING | \n",
      "encoder.pooler.dense.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PHASE A (freeze encoder, fast) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='528' max='528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [528/528 34:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.533872</td>\n",
       "      <td>2.683761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.584024</td>\n",
       "      <td>2.363312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.242824</td>\n",
       "      <td>2.159060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.049827</td>\n",
       "      <td>2.101222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VAL RESULTS (Phase A) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1012215614318848, 'eval_runtime': 50.4296, 'eval_samples_per_second': 1.289, 'eval_steps_per_second': 0.337, 'epoch': 4.0}\n",
      "\n",
      "===== PHASE B (unfreeze encoder, benchmark) =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [133/396 21:12 < 42:35, 0.10 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 01:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 266\u001b[0m\n\u001b[0;32m    256\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    257\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    258\u001b[0m     args\u001b[38;5;241m=\u001b[39margs_B,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,  \u001b[38;5;66;03m# ✅ Phase B metrics\u001b[39;00m\n\u001b[0;32m    263\u001b[0m )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== PHASE B (unfreeze encoder, benchmark) =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 266\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== FINAL VAL RESULTS (TrOCR) =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mevaluate())\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2174\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2172\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2641\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2641\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m   2643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2647\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:3020\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3018\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3020\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3021\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3023\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2969\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2968\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2969\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2972\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:4289\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4285\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n\u001b[0;32m   4287\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 4289\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4290\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4293\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4299\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model_preparation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:4569\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4567\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4568\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4569\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4572\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4573\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[4], line 157\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n\u001b[0;32m    156\u001b[0m     preds, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m--> 157\u001b[0m     pred_str \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    160\u001b[0m     labels[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\processing_utils.py:1600\u001b[0m, in \u001b[0;36mProcessorMixin.batch_decode\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot batch decode text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2957\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;124;03mConvert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   2938\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2954\u001b[0m \u001b[38;5;124;03m    `list[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;66;03m# Forward to decode() which now handles batched input natively\u001b[39;00m\n\u001b[1;32m-> 2957\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   2958\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39msequences,\n\u001b[0;32m   2959\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2960\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   2961\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2962\u001b[0m )\n\u001b[0;32m   2963\u001b[0m \u001b[38;5;66;03m# Ensure we always return a list for backwards compatibility\u001b[39;00m\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2913\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   2912\u001b[0m     clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_up_tokenization_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 2913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   2914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2915\u001b[0m             token_ids\u001b[38;5;241m=\u001b[39mseq,\n\u001b[0;32m   2916\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2917\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   2918\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2919\u001b[0m         )\n\u001b[0;32m   2920\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m token_ids\n\u001b[0;32m   2921\u001b[0m     ]\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2924\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   2925\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2927\u001b[0m )\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   2912\u001b[0m     clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_up_tokenization_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 2914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2915\u001b[0m             token_ids\u001b[38;5;241m=\u001b[39mseq,\n\u001b[0;32m   2916\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2917\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   2918\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2919\u001b[0m         )\n\u001b[0;32m   2920\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m token_ids\n\u001b[0;32m   2921\u001b[0m     ]\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2924\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   2925\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2927\u001b[0m )\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_tokenizers.py:929\u001b[0m, in \u001b[0;36mTokenizersBackend._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    928\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m token_ids[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 929\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    932\u001b[0m     clean_up_tokenization_spaces\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[0;32m    935\u001b[0m )\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;66;03m# Call custom cleanup method if it exists\u001b[39;00m\n",
      "\u001b[1;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TrOCR CAPTCHA PIPELINE — ONE BLOCK (CPU friendly, HF-safe)\n",
    "# Train/Val split + Augmentation + Phase A (fast) + Phase B (benchmark)\n",
    "# Metrics: CER + Exact (Phase B only)\n",
    "# =========================================================\n",
    "\n",
    "import os, random, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    default_data_collator,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 0) CONFIG\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\jbche\\OneDrive - Université Paris 1 Panthéon-Sorbonne\\MOSEF\\projets\\webscrapping\\projet\\data\\data_OCR_Captcha-20260117T105614Z-1-001\\finetune_russie\\data_russie\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_DIR, \"val\")\n",
    "\n",
    "TRAIN_FRAC = 0.9\n",
    "MAX_LEN = 6  # captcha souvent 4-6 -> accélère la génération\n",
    "\n",
    "MODEL_CKPT = \"microsoft/trocr-small-printed\"\n",
    "OUT_DIR_A = \"./trocr_phaseA\"\n",
    "OUT_DIR_B = \"./trocr_phaseB\"\n",
    "FINAL_DIR = \"./trocr_final\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) TRAIN / VAL SPLIT (COPY)\n",
    "# -------------------------\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "if len(os.listdir(TRAIN_DIR)) == 0 and len(os.listdir(VAL_DIR)) == 0:\n",
    "    files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))]\n",
    "    random.shuffle(files)\n",
    "    n_train = int(TRAIN_FRAC * len(files))\n",
    "\n",
    "    train_files = files[:n_train]\n",
    "    val_files   = files[n_train:]\n",
    "\n",
    "    for f in train_files:\n",
    "        shutil.copy(os.path.join(DATA_DIR, f), os.path.join(TRAIN_DIR, f))\n",
    "    for f in val_files:\n",
    "        shutil.copy(os.path.join(DATA_DIR, f), os.path.join(VAL_DIR, f))\n",
    "\n",
    "print(\"Train:\", len([f for f in os.listdir(TRAIN_DIR) if f.lower().endswith(('.png','.jpg','.jpeg'))]))\n",
    "print(\"Val  :\", len([f for f in os.listdir(VAL_DIR) if f.lower().endswith(('.png','.jpg','.jpeg'))]))\n",
    "\n",
    "# -------------------------\n",
    "# 2) BUILD LABEL DFS (label = filename stem)\n",
    "# -------------------------\n",
    "def build_df(folder):\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))]\n",
    "    df = pd.DataFrame(files, columns=[\"file_name\"])\n",
    "    df[\"text\"] = df[\"file_name\"].apply(lambda x: x.rsplit(\".\", 1)[0].lower())\n",
    "    return df\n",
    "\n",
    "train_df = build_df(TRAIN_DIR)\n",
    "val_df   = build_df(VAL_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# 3) AUGMENTATION (captcha-ish)\n",
    "# -------------------------\n",
    "def augment_image(img: Image.Image) -> Image.Image:\n",
    "    if random.random() < 0.5:\n",
    "        img = ImageEnhance.Brightness(img).enhance(random.uniform(0.8, 1.2))\n",
    "    if random.random() < 0.5:\n",
    "        img = ImageEnhance.Contrast(img).enhance(random.uniform(0.8, 1.3))\n",
    "    if random.random() < 0.2:\n",
    "        arr = np.array(img).astype(np.float32)\n",
    "        noise = np.random.normal(0, 5, arr.shape)\n",
    "        arr = np.clip(arr + noise, 0, 255).astype(np.uint8)\n",
    "        img = Image.fromarray(arr)\n",
    "    if random.random() < 0.15:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.3, 0.8)))\n",
    "    return img\n",
    "\n",
    "# -------------------------\n",
    "# 4) PROCESSOR\n",
    "# -------------------------\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "# -------------------------\n",
    "# 5) DATASET (robust pixel_values indexing)\n",
    "# -------------------------\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, folder, df, processor, max_length=6, augment=False):\n",
    "        self.folder = folder\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df.loc[idx, \"file_name\"]\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "\n",
    "        img_path = os.path.join(self.folder, file_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.augment:\n",
    "            image = augment_image(image)\n",
    "\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values[0]  # ✅ safe\n",
    "\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "        labels = [l if l != self.processor.tokenizer.pad_token_id else -100 for l in labels]\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "train_ds = CaptchaDataset(TRAIN_DIR, train_df, processor, max_length=MAX_LEN, augment=True)\n",
    "val_ds   = CaptchaDataset(VAL_DIR, val_df, processor, max_length=MAX_LEN, augment=False)\n",
    "\n",
    "# -------------------------\n",
    "# 6) METRICS (Phase B only)\n",
    "# -------------------------\n",
    "def levenshtein(a: str, b: str) -> int:\n",
    "    n, m = len(a), len(b)\n",
    "    if n == 0: return m\n",
    "    if m == 0: return n\n",
    "    prev = list(range(m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        cur = [i] + [0] * m\n",
    "        ai = a[i - 1]\n",
    "        for j in range(1, m + 1):\n",
    "            cost = 0 if ai == b[j - 1] else 1\n",
    "            cur[j] = min(prev[j] + 1, cur[j - 1] + 1, prev[j - 1] + cost)\n",
    "        prev = cur\n",
    "    return prev[m]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    pred_str = processor.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = labels.copy()\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    exact = float(np.mean([p == l for p, l in zip(pred_str, label_str)]))\n",
    "\n",
    "    edits, chars = 0, 0\n",
    "    for p, l in zip(pred_str, label_str):\n",
    "        edits += levenshtein(p, l)\n",
    "        chars += len(l)\n",
    "\n",
    "    cer = edits / max(1, chars)\n",
    "    return {\"exact_acc\": exact, \"cer\": cer}\n",
    "\n",
    "# -------------------------\n",
    "# 7) MODEL + HF-SAFE GENERATION CONFIG\n",
    "# -------------------------\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "model.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "model.generation_config.max_length = MAX_LEN\n",
    "model.generation_config.num_beams = 1  # greedy\n",
    "\n",
    "# =========================================================\n",
    "# 8) PHASE A — FAST ADAPT (freeze encoder, loss only, NO generate)\n",
    "# =========================================================\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "args_A = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR_A,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=4,\n",
    "    warmup_steps=20,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    predict_with_generate=False,     # ✅ faster\n",
    "    logging_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args_A,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=None,            # ✅ Phase A = loss only\n",
    ")\n",
    "\n",
    "print(\"\\n===== PHASE A (freeze encoder, fast) =====\")\n",
    "trainer.train()\n",
    "print(\"\\n===== VAL RESULTS (Phase A) =====\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "# =========================================================\n",
    "# 9) PHASE B — BENCHMARK (unfreeze, generate ON, CER/Exact)\n",
    "# =========================================================\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "args_B = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR_B,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=10,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    predict_with_generate=True,      # ✅ metrics need generate()\n",
    "    logging_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args_B,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,  # ✅ Phase B metrics\n",
    ")\n",
    "\n",
    "print(\"\\n===== PHASE B (unfreeze encoder, benchmark) =====\")\n",
    "trainer.train()\n",
    "print(\"\\n===== FINAL VAL RESULTS (TrOCR) =====\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "trainer.save_model(FINAL_DIR)\n",
    "processor.save_pretrained(FINAL_DIR)\n",
    "print(f\"\\nSaved model+processor to: {FINAL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e00563-ef09-4f1a-9617-5a62656d610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 362/362 [00:01<00:00, 195.90it/s, Materializing param=encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== PHASE B =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='396' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [396/396 1:09:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Acc</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.406790</td>\n",
       "      <td>0.995753</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.290196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.855281</td>\n",
       "      <td>0.869637</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.239216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.600352</td>\n",
       "      <td>0.807607</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.243137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FINAL RESULTS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 01:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8696371912956238, 'eval_exact_acc': 0.4, 'eval_cer': 0.23921568627450981, 'eval_runtime': 105.9611, 'eval_samples_per_second': 0.613, 'eval_steps_per_second': 0.16, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|█████████████████████████████| 1/1 [00:00<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./trocr_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# PHASE B — continue finetuning from Phase A\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    default_data_collator,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# -------- CONFIG --------\n",
    "BASE_MODEL = \"microsoft/trocr-small-printed\"\n",
    "MODEL_A_DIR = \"trocr_phaseA/checkpoint-528\"   # 🔥 ton checkpoint\n",
    "OUT_DIR_B = \"./trocr_phaseB\"\n",
    "MAX_LEN = 6\n",
    "\n",
    "# -------- LOAD MODEL + PROCESSOR --------\n",
    "processor = TrOCRProcessor.from_pretrained(BASE_MODEL)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_A_DIR)\n",
    "\n",
    "model.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "model.generation_config.max_length = MAX_LEN\n",
    "model.generation_config.num_beams = 1\n",
    "\n",
    "# -------- UNFREEZE ENCODER --------\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# -------- METRICS --------\n",
    "def levenshtein(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    if n == 0: return m\n",
    "    if m == 0: return n\n",
    "    prev = list(range(m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        cur = [i] + [0] * m\n",
    "        for j in range(1, m + 1):\n",
    "            cost = 0 if a[i-1] == b[j-1] else 1\n",
    "            cur[j] = min(prev[j] + 1, cur[j-1] + 1, prev[j-1] + cost)\n",
    "        prev = cur\n",
    "    return prev[m]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # preds may contain -100 -> cast to int\n",
    "    preds = np.clip(preds, 0, processor.tokenizer.vocab_size - 1)\n",
    "\n",
    "    pred_str = processor.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = labels.copy()\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    exact = np.mean([p == l for p, l in zip(pred_str, label_str)])\n",
    "\n",
    "    edits, chars = 0, 0\n",
    "    for p, l in zip(pred_str, label_str):\n",
    "        edits += levenshtein(p, l)\n",
    "        chars += len(l)\n",
    "\n",
    "    cer = edits / max(1, chars)\n",
    "    return {\"exact_acc\": exact, \"cer\": cer}\n",
    "\n",
    "# -------- TRAINING ARGS --------\n",
    "args_B = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR_B,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=10,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args_B,\n",
    "    train_dataset=train_ds,   # ⚠️ tu gardes tes datasets existants\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"===== PHASE B =====\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"===== FINAL RESULTS =====\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "trainer.save_model(\"./trocr_final\")\n",
    "print(\"Model saved in ./trocr_final\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
