{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b517310",
   "metadata": {},
   "source": [
    "# PIPELINE COMPLETE FINE TUNE TR OCR MEME STRATEGIE QUE LE MODEL FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115e964-c053-43cc-99b5-a3ecac372ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 527\n",
      "Val  : 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆ| 360/360 [00:01<00:00, 188.99it/s, Materializing param=encoder\n",
      "VisionEncoderDecoderModel LOAD REPORT from: microsoft/trocr-small-printed\n",
      "Key                         | Status  | \n",
      "----------------------------+---------+-\n",
      "encoder.pooler.dense.bias   | MISSING | \n",
      "encoder.pooler.dense.weight | MISSING | \n",
      "\n",
      "Notes:\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PHASE A (freeze encoder, fast) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='528' max='528' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [528/528 34:36, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.533872</td>\n",
       "      <td>2.683761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.584024</td>\n",
       "      <td>2.363312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.242824</td>\n",
       "      <td>2.159060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.049827</td>\n",
       "      <td>2.101222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.05it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== VAL RESULTS (Phase A) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.1012215614318848, 'eval_runtime': 50.4296, 'eval_samples_per_second': 1.289, 'eval_steps_per_second': 0.337, 'epoch': 4.0}\n",
      "\n",
      "===== PHASE B (unfreeze encoder, benchmark) =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [133/396 21:12 < 42:35, 0.10 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 01:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OverflowError",
     "evalue": "can't convert negative int to unsigned",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 266\u001b[0m\n\u001b[0;32m    256\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    257\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    258\u001b[0m     args\u001b[38;5;241m=\u001b[39margs_B,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,  \u001b[38;5;66;03m# âœ… Phase B metrics\u001b[39;00m\n\u001b[0;32m    263\u001b[0m )\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== PHASE B (unfreeze encoder, benchmark) =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 266\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== FINAL VAL RESULTS (TrOCR) =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mevaluate())\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2174\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2172\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2641\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2638\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2641\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m   2643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2647\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:3020\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[0;32m   3018\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 3020\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3021\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   3023\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:2969\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[0;32m   2968\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m-> 2969\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2970\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2972\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:4289\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4285\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n\u001b[0;32m   4287\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 4289\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluation_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4290\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4292\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4293\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4299\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_model_preparation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\trainer.py:4569\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4567\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4568\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4569\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_set_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4571\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4572\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4573\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[4], line 157\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_metrics\u001b[39m(eval_pred):\n\u001b[0;32m    156\u001b[0m     preds, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m--> 157\u001b[0m     pred_str \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    160\u001b[0m     labels[labels \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m] \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\processing_utils.py:1600\u001b[0m, in \u001b[0;36mProcessorMixin.batch_decode\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot batch decode text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2957\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   2936\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2937\u001b[0m \u001b[38;5;124;03mConvert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[0;32m   2938\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2954\u001b[0m \u001b[38;5;124;03m    `list[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2956\u001b[0m \u001b[38;5;66;03m# Forward to decode() which now handles batched input natively\u001b[39;00m\n\u001b[1;32m-> 2957\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[0;32m   2958\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39msequences,\n\u001b[0;32m   2959\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2960\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   2961\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2962\u001b[0m )\n\u001b[0;32m   2963\u001b[0m \u001b[38;5;66;03m# Ensure we always return a list for backwards compatibility\u001b[39;00m\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2913\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   2912\u001b[0m     clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_up_tokenization_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 2913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m   2914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2915\u001b[0m             token_ids\u001b[38;5;241m=\u001b[39mseq,\n\u001b[0;32m   2916\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2917\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   2918\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2919\u001b[0m         )\n\u001b[0;32m   2920\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m token_ids\n\u001b[0;32m   2921\u001b[0m     ]\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2924\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   2925\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2927\u001b[0m )\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2914\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(token_ids) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   2912\u001b[0m     clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_up_tokenization_spaces\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   2913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m-> 2914\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2915\u001b[0m             token_ids\u001b[38;5;241m=\u001b[39mseq,\n\u001b[0;32m   2916\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2917\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   2918\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2919\u001b[0m         )\n\u001b[0;32m   2920\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m token_ids\n\u001b[0;32m   2921\u001b[0m     ]\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   2924\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   2925\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   2926\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2927\u001b[0m )\n",
      "File \u001b[1;32mC:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_tokenizers.py:929\u001b[0m, in \u001b[0;36mTokenizersBackend._decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    928\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m token_ids[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 929\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    932\u001b[0m     clean_up_tokenization_spaces\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    934\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[0;32m    935\u001b[0m )\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;66;03m# Call custom cleanup method if it exists\u001b[39;00m\n",
      "\u001b[1;31mOverflowError\u001b[0m: can't convert negative int to unsigned"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# TrOCR CAPTCHA PIPELINE â€” ONE BLOCK (CPU friendly, HF-safe)\n",
    "# Train/Val split + Augmentation + Phase A (fast) + Phase B (benchmark)\n",
    "# Metrics: CER + Exact (Phase B only)\n",
    "# =========================================================\n",
    "\n",
    "import os, random, shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    default_data_collator,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 0) CONFIG\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\jbche\\OneDrive - UniversitÃ© Paris 1 PanthÃ©on-Sorbonne\\MOSEF\\projets\\webscrapping\\projet\\data\\data_OCR_Captcha-20260117T105614Z-1-001\\finetune_russie\\data_russie\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR   = os.path.join(DATA_DIR, \"val\")\n",
    "\n",
    "TRAIN_FRAC = 0.9\n",
    "MAX_LEN = 6  # captcha souvent 4-6 -> accÃ©lÃ¨re la gÃ©nÃ©ration\n",
    "\n",
    "MODEL_CKPT = \"microsoft/trocr-small-printed\"\n",
    "OUT_DIR_A = \"./trocr_phaseA\"\n",
    "OUT_DIR_B = \"./trocr_phaseB\"\n",
    "FINAL_DIR = \"./trocr_final\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) TRAIN / VAL SPLIT (COPY)\n",
    "# -------------------------\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(VAL_DIR, exist_ok=True)\n",
    "\n",
    "if len(os.listdir(TRAIN_DIR)) == 0 and len(os.listdir(VAL_DIR)) == 0:\n",
    "    files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))]\n",
    "    random.shuffle(files)\n",
    "    n_train = int(TRAIN_FRAC * len(files))\n",
    "\n",
    "    train_files = files[:n_train]\n",
    "    val_files   = files[n_train:]\n",
    "\n",
    "    for f in train_files:\n",
    "        shutil.copy(os.path.join(DATA_DIR, f), os.path.join(TRAIN_DIR, f))\n",
    "    for f in val_files:\n",
    "        shutil.copy(os.path.join(DATA_DIR, f), os.path.join(VAL_DIR, f))\n",
    "\n",
    "print(\"Train:\", len([f for f in os.listdir(TRAIN_DIR) if f.lower().endswith(('.png','.jpg','.jpeg'))]))\n",
    "print(\"Val  :\", len([f for f in os.listdir(VAL_DIR) if f.lower().endswith(('.png','.jpg','.jpeg'))]))\n",
    "\n",
    "# -------------------------\n",
    "# 2) BUILD LABEL DFS (label = filename stem)\n",
    "# -------------------------\n",
    "def build_df(folder):\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith((\".png\",\".jpg\",\".jpeg\"))]\n",
    "    df = pd.DataFrame(files, columns=[\"file_name\"])\n",
    "    df[\"text\"] = df[\"file_name\"].apply(lambda x: x.rsplit(\".\", 1)[0].lower())\n",
    "    return df\n",
    "\n",
    "train_df = build_df(TRAIN_DIR)\n",
    "val_df   = build_df(VAL_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# 3) AUGMENTATION (captcha-ish)\n",
    "# -------------------------\n",
    "def augment_image(img: Image.Image) -> Image.Image:\n",
    "    if random.random() < 0.5:\n",
    "        img = ImageEnhance.Brightness(img).enhance(random.uniform(0.8, 1.2))\n",
    "    if random.random() < 0.5:\n",
    "        img = ImageEnhance.Contrast(img).enhance(random.uniform(0.8, 1.3))\n",
    "    if random.random() < 0.2:\n",
    "        arr = np.array(img).astype(np.float32)\n",
    "        noise = np.random.normal(0, 5, arr.shape)\n",
    "        arr = np.clip(arr + noise, 0, 255).astype(np.uint8)\n",
    "        img = Image.fromarray(arr)\n",
    "    if random.random() < 0.15:\n",
    "        img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.3, 0.8)))\n",
    "    return img\n",
    "\n",
    "# -------------------------\n",
    "# 4) PROCESSOR\n",
    "# -------------------------\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "# -------------------------\n",
    "# 5) DATASET (robust pixel_values indexing)\n",
    "# -------------------------\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, folder, df, processor, max_length=6, augment=False):\n",
    "        self.folder = folder\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df.loc[idx, \"file_name\"]\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "\n",
    "        img_path = os.path.join(self.folder, file_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.augment:\n",
    "            image = augment_image(image)\n",
    "\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values[0]  # âœ… safe\n",
    "\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        ).input_ids\n",
    "        labels = [l if l != self.processor.tokenizer.pad_token_id else -100 for l in labels]\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": torch.tensor(labels, dtype=torch.long)}\n",
    "\n",
    "train_ds = CaptchaDataset(TRAIN_DIR, train_df, processor, max_length=MAX_LEN, augment=True)\n",
    "val_ds   = CaptchaDataset(VAL_DIR, val_df, processor, max_length=MAX_LEN, augment=False)\n",
    "\n",
    "# -------------------------\n",
    "# 6) METRICS (Phase B only)\n",
    "# -------------------------\n",
    "def levenshtein(a: str, b: str) -> int:\n",
    "    n, m = len(a), len(b)\n",
    "    if n == 0: return m\n",
    "    if m == 0: return n\n",
    "    prev = list(range(m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        cur = [i] + [0] * m\n",
    "        ai = a[i - 1]\n",
    "        for j in range(1, m + 1):\n",
    "            cost = 0 if ai == b[j - 1] else 1\n",
    "            cur[j] = min(prev[j] + 1, cur[j - 1] + 1, prev[j - 1] + cost)\n",
    "        prev = cur\n",
    "    return prev[m]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    pred_str = processor.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = labels.copy()\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    exact = float(np.mean([p == l for p, l in zip(pred_str, label_str)]))\n",
    "\n",
    "    edits, chars = 0, 0\n",
    "    for p, l in zip(pred_str, label_str):\n",
    "        edits += levenshtein(p, l)\n",
    "        chars += len(l)\n",
    "\n",
    "    cer = edits / max(1, chars)\n",
    "    return {\"exact_acc\": exact, \"cer\": cer}\n",
    "\n",
    "# -------------------------\n",
    "# 7) MODEL + HF-SAFE GENERATION CONFIG\n",
    "# -------------------------\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "\n",
    "model.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "model.generation_config.max_length = MAX_LEN\n",
    "model.generation_config.num_beams = 1  # greedy\n",
    "\n",
    "# =========================================================\n",
    "# 8) PHASE A â€” FAST ADAPT (freeze encoder, loss only, NO generate)\n",
    "# =========================================================\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "args_A = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR_A,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=4,\n",
    "    warmup_steps=20,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    predict_with_generate=False,     \n",
    "    logging_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args_A,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=None,            \n",
    ")\n",
    "\n",
    "print(\"\\n===== PHASE A (freeze encoder, fast) =====\")\n",
    "trainer.train()\n",
    "print(\"\\n===== VAL RESULTS (Phase A) =====\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "# =========================================================\n",
    "# 9) PHASE B â€” BENCHMARK (unfreeze, generate ON, CER/Exact)\n",
    "# =========================================================\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "args_B = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR_B,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=10,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    predict_with_generate=True,      \n",
    "    logging_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args_B,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,  \n",
    ")\n",
    "\n",
    "print(\"\\n===== PHASE B (unfreeze encoder, benchmark) =====\")\n",
    "trainer.train()\n",
    "print(\"\\n===== FINAL VAL RESULTS (TrOCR) =====\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "trainer.save_model(FINAL_DIR)\n",
    "processor.save_pretrained(FINAL_DIR)\n",
    "print(f\"\\nSaved model+processor to: {FINAL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e00563-ef09-4f1a-9617-5a62656d610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|â–ˆ| 362/362 [00:01<00:00, 195.90it/s, Materializing param=encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== PHASE B =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='396' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [396/396 1:09:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Exact Acc</th>\n",
       "      <th>Cer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.406790</td>\n",
       "      <td>0.995753</td>\n",
       "      <td>0.261538</td>\n",
       "      <td>0.290196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.855281</td>\n",
       "      <td>0.869637</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.239216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.600352</td>\n",
       "      <td>0.807607</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.243137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.44s/it]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.14it/s]\n",
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FINAL RESULTS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Mosef\\webscrapping\\projet\\Captchas-Automatic-Resolution\\.venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 01:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8696371912956238, 'eval_exact_acc': 0.4, 'eval_cer': 0.23921568627450981, 'eval_runtime': 105.9611, 'eval_samples_per_second': 0.613, 'eval_steps_per_second': 0.16, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in ./trocr_final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# PHASE B â€” continue finetuning from Phase A\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    TrOCRProcessor,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    default_data_collator,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "# -------- CONFIG --------\n",
    "BASE_MODEL = \"microsoft/trocr-small-printed\"\n",
    "MODEL_A_DIR = \"trocr_phaseA/checkpoint-528\"   # ðŸ”¥ ton checkpoint\n",
    "OUT_DIR_B = \"./trocr_phaseB\"\n",
    "MAX_LEN = 6\n",
    "\n",
    "# -------- LOAD MODEL + PROCESSOR --------\n",
    "processor = TrOCRProcessor.from_pretrained(BASE_MODEL)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_A_DIR)\n",
    "\n",
    "model.generation_config = GenerationConfig.from_model_config(model.config)\n",
    "model.generation_config.max_length = MAX_LEN\n",
    "model.generation_config.num_beams = 1\n",
    "\n",
    "# -------- UNFREEZE ENCODER --------\n",
    "for p in model.encoder.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# -------- METRICS --------\n",
    "def levenshtein(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    if n == 0: return m\n",
    "    if m == 0: return n\n",
    "    prev = list(range(m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        cur = [i] + [0] * m\n",
    "        for j in range(1, m + 1):\n",
    "            cost = 0 if a[i-1] == b[j-1] else 1\n",
    "            cur[j] = min(prev[j] + 1, cur[j-1] + 1, prev[j-1] + cost)\n",
    "        prev = cur\n",
    "    return prev[m]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # preds may contain -100 -> cast to int\n",
    "    preds = np.clip(preds, 0, processor.tokenizer.vocab_size - 1)\n",
    "\n",
    "    pred_str = processor.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = labels.copy()\n",
    "    labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    exact = np.mean([p == l for p, l in zip(pred_str, label_str)])\n",
    "\n",
    "    edits, chars = 0, 0\n",
    "    for p, l in zip(pred_str, label_str):\n",
    "        edits += levenshtein(p, l)\n",
    "        chars += len(l)\n",
    "\n",
    "    cer = edits / max(1, chars)\n",
    "    return {\"exact_acc\": exact, \"cer\": cer}\n",
    "\n",
    "# -------- TRAINING ARGS --------\n",
    "args_B = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR_B,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=3,\n",
    "    warmup_steps=10,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "\n",
    "    predict_with_generate=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "\n",
    "    fp16=False,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args_B,\n",
    "    train_dataset=train_ds,   \n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"===== PHASE B =====\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"===== FINAL RESULTS =====\")\n",
    "print(trainer.evaluate())\n",
    "\n",
    "trainer.save_model(\"./trocr_final\")\n",
    "print(\"Model saved in ./trocr_final\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
